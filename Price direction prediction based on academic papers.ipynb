{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e7cea3",
   "metadata": {},
   "source": [
    "## Citations-\n",
    "\n",
    "Deep LOB algorithm for raw only engineered mode is derived from-:  \n",
    "Z. Zhang, S. Zohren and S. Roberts, \"DeepLOB: Deep Convolutional Neural Networks for Limit Order Books,\" in IEEE Transactions on Signal Processing, vol. 67, no. 11, pp. 3001-3012, 1 June1, 2019, doi: 10.1109/TSP.2019.2907260.\n",
    "\n",
    "Algorithm for the engineered features input mode is derived from-:     \n",
    "Avraam Tsantekidis, Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, Alexandros Iosifidis,\n",
    "Using Deep Learning for price prediction by exploiting stationary limit order book features,\n",
    "Applied Soft Computing,\n",
    "Volume 93,\n",
    "2020,\n",
    "106401,\n",
    "ISSN 1568-4946,\n",
    "https://doi.org/10.1016/j.asoc.2020.106401."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15016ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d387a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_mode = \"raw\" #\"raw\" (DeepLOB) or \"feat\" (Tsantekidis et al.)\n",
    "window = 50\n",
    "stride = 10\n",
    "\n",
    "# for 3-class labeling\n",
    "k_smooth  = 5 # moving-average window (number of events) for past & future mid-price\n",
    "alpha_bps = 0.1 # threshold alpha in basis points; tune to balance classes (e.g., 0.1 to 5.0)\n",
    "h_fwd = 20\n",
    "\n",
    "# Training epochs\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31e22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_p = [4*i for i in range(10)]\n",
    "ask_v = [4*i+1 for i in range(10)]\n",
    "bid_p = [4*i+2 for i in range(10)]\n",
    "bid_v = [4*i+3 for i in range(10)]\n",
    "\n",
    "ob = pd.read_csv(\"AMZN_2012-06-21_34200000_57600000_orderbook_10.csv\", header=None)\n",
    "\n",
    "a_px = ob[ask_p].add_prefix(\"ask_price\")\n",
    "a_sz = ob[ask_v].add_prefix(\"ask_size\")\n",
    "b_px = ob[bid_p].add_prefix(\"bid_price\")\n",
    "b_sz = ob[bid_v].add_prefix(\"bid_size\")\n",
    "\n",
    "mid = 0.5*(a_px.iloc[:,0] + b_px.iloc[:,0]).astype(float)\n",
    "spread = a_px.iloc[:,0] - b_px.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b084d4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>2230700</td>\n",
       "      <td>200</td>\n",
       "      <td>2240000</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>2202500</td>\n",
       "      <td>5000</td>\n",
       "      <td>2294300</td>\n",
       "      <td>100</td>\n",
       "      <td>2202000</td>\n",
       "      <td>100</td>\n",
       "      <td>2298000</td>\n",
       "      <td>100</td>\n",
       "      <td>2189700</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2240000</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>2204000</td>\n",
       "      <td>100</td>\n",
       "      <td>2294300</td>\n",
       "      <td>100</td>\n",
       "      <td>2202500</td>\n",
       "      <td>5000</td>\n",
       "      <td>2298000</td>\n",
       "      <td>100</td>\n",
       "      <td>2202000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>2204000</td>\n",
       "      <td>100</td>\n",
       "      <td>2267700</td>\n",
       "      <td>100</td>\n",
       "      <td>2202500</td>\n",
       "      <td>5000</td>\n",
       "      <td>2294300</td>\n",
       "      <td>100</td>\n",
       "      <td>2202000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2237500</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>2213000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2267700</td>\n",
       "      <td>100</td>\n",
       "      <td>2204000</td>\n",
       "      <td>100</td>\n",
       "      <td>2294300</td>\n",
       "      <td>100</td>\n",
       "      <td>2202500</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2237500</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>2213000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2267700</td>\n",
       "      <td>100</td>\n",
       "      <td>2204000</td>\n",
       "      <td>100</td>\n",
       "      <td>2294300</td>\n",
       "      <td>100</td>\n",
       "      <td>2202500</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269743</th>\n",
       "      <td>2206200</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>2204300</td>\n",
       "      <td>2300</td>\n",
       "      <td>2207600</td>\n",
       "      <td>100</td>\n",
       "      <td>2204200</td>\n",
       "      <td>100</td>\n",
       "      <td>2207900</td>\n",
       "      <td>2300</td>\n",
       "      <td>2204100</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269744</th>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>2204300</td>\n",
       "      <td>2300</td>\n",
       "      <td>2207900</td>\n",
       "      <td>2300</td>\n",
       "      <td>2204200</td>\n",
       "      <td>100</td>\n",
       "      <td>2208000</td>\n",
       "      <td>3100</td>\n",
       "      <td>2204100</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269745</th>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>2204300</td>\n",
       "      <td>2300</td>\n",
       "      <td>2208000</td>\n",
       "      <td>3100</td>\n",
       "      <td>2204200</td>\n",
       "      <td>100</td>\n",
       "      <td>2208100</td>\n",
       "      <td>1700</td>\n",
       "      <td>2204100</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269746</th>\n",
       "      <td>2206300</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>2204300</td>\n",
       "      <td>2300</td>\n",
       "      <td>2207900</td>\n",
       "      <td>2300</td>\n",
       "      <td>2204200</td>\n",
       "      <td>100</td>\n",
       "      <td>2208000</td>\n",
       "      <td>3100</td>\n",
       "      <td>2204100</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269747</th>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>2204300</td>\n",
       "      <td>2300</td>\n",
       "      <td>2208000</td>\n",
       "      <td>3100</td>\n",
       "      <td>2204200</td>\n",
       "      <td>100</td>\n",
       "      <td>2208100</td>\n",
       "      <td>1700</td>\n",
       "      <td>2204100</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269748 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1        2    3        4     5        6    7        8   \\\n",
       "0       2239500  100  2231800  100  2239900   100  2230700  200  2240000   \n",
       "1       2239500  100  2238100   21  2239900   100  2231800  100  2240000   \n",
       "2       2239500  100  2238100   21  2239600    20  2231800  100  2239900   \n",
       "3       2239500  100  2238100   21  2239600    20  2237500  100  2239900   \n",
       "4       2239500  100  2238100   21  2239600    20  2237500  100  2239900   \n",
       "...         ...  ...      ...  ...      ...   ...      ...  ...      ...   \n",
       "269743  2206200  100  2205100  249  2206400   100  2205000   71  2206500   \n",
       "269744  2206400  100  2205100  249  2206500  1290  2205000   71  2206700   \n",
       "269745  2206400  100  2205100  249  2206500  1290  2205000   71  2206700   \n",
       "269746  2206300  100  2205100  249  2206400   100  2205000   71  2206500   \n",
       "269747  2206400  100  2205100  249  2206500  1290  2205000   71  2206700   \n",
       "\n",
       "          9   ...       30    31       32    33       34    35       36    37  \\\n",
       "0        220  ...  2202500  5000  2294300   100  2202000   100  2298000   100   \n",
       "1        220  ...  2204000   100  2294300   100  2202500  5000  2298000   100   \n",
       "2        100  ...  2204000   100  2267700   100  2202500  5000  2294300   100   \n",
       "3        100  ...  2213000  4000  2267700   100  2204000   100  2294300   100   \n",
       "4        100  ...  2213000  4000  2267700   100  2204000   100  2294300   100   \n",
       "...      ...  ...      ...   ...      ...   ...      ...   ...      ...   ...   \n",
       "269743  1290  ...  2204300  2300  2207600   100  2204200   100  2207900  2300   \n",
       "269744   170  ...  2204300  2300  2207900  2300  2204200   100  2208000  3100   \n",
       "269745   170  ...  2204300  2300  2208000  3100  2204200   100  2208100  1700   \n",
       "269746  1290  ...  2204300  2300  2207900  2300  2204200   100  2208000  3100   \n",
       "269747   170  ...  2204300  2300  2208000  3100  2204200   100  2208100  1700   \n",
       "\n",
       "             38    39  \n",
       "0       2189700   100  \n",
       "1       2202000   100  \n",
       "2       2202000   100  \n",
       "3       2202500  5000  \n",
       "4       2202500  5000  \n",
       "...         ...   ...  \n",
       "269743  2204100  3300  \n",
       "269744  2204100  3300  \n",
       "269745  2204100  3300  \n",
       "269746  2204100  3300  \n",
       "269747  2204100  3300  \n",
       "\n",
       "[269748 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9185b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.DataFrame(index=ob.index)\n",
    "if engineered_mode == \"feat\":\n",
    "    # stationary transforms\n",
    "    feat[\"mid_ret\"] = mid.pct_change().fillna(0.0)\n",
    "    # relative prices vs mid(per level)\n",
    "    for lvl in range(10):\n",
    "        feat[f\"ask_pdiff{lvl}\"] = a_px.iloc[:,lvl] / mid - 1.0\n",
    "        feat[f\"bid_pdiff{lvl}\"] = b_px.iloc[:,lvl] / mid - 1.0\n",
    "    # depth aggregates\n",
    "    for k in (1,3,5,10):\n",
    "        feat[f\"depth_ask{k}\"] = a_sz.iloc[:,:k].sum(1)\n",
    "        feat[f\"depth_bid{k}\"] = b_sz.iloc[:,:k].sum(1)\n",
    "    # L1 imbalance\n",
    "    feat[\"qi_lvl1\"] = b_sz.iloc[:,0] / (a_sz.iloc[:,0] + b_sz.iloc[:,0] + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df12c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating X according to input mode\n",
    "if engineered_mode == \"raw\":\n",
    "    Xdf = pd.concat([a_px, a_sz, b_px, b_sz], axis=1)   # 40 columns\n",
    "elif engineered_mode == \"feat\":\n",
    "    Xdf = feat\n",
    "else:\n",
    "    raise ValueError(\"engineered_mode must be 'raw' or 'feat'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdbe2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution (Down/Neutral/Up): {0: 100512, 1: 75583, 2: 93633}\n",
      "Kept 269728 rows after horizon/smoothing trimming.\n"
     ]
    }
   ],
   "source": [
    "# creating 3 class labels\n",
    "# Moving averages of past and future mid-price\n",
    "alpha = alpha_bps / 1e4  #(1 bps = 0.0001)\n",
    "\n",
    "m_past = mid.shift(1).rolling(k_smooth, min_periods=1).mean()\n",
    "\n",
    "m_fut  = mid.shift(-1).rolling(k_smooth, min_periods=1).mean()\n",
    "\n",
    "m_fut_H = m_fut.shift(-(h_fwd-1))\n",
    "\n",
    "m_base  = m_past.replace(0, np.nan).ffill().bfill()\n",
    "delta   = (m_fut_H - m_past) / m_base\n",
    "\n",
    "valid_mask = (~m_past.isna()) & (~m_fut_H.isna()) & (~delta.isna())\n",
    "valid_idx  = mid.index[valid_mask]\n",
    "\n",
    "# Build labels: 0=Down, 1=Neutral, 2=Up\n",
    "y3 = pd.Series(1, index=valid_idx, dtype=np.int64)   # default Neutral\n",
    "y3.loc[delta.loc[valid_idx] >  alpha] = 2\n",
    "y3.loc[delta.loc[valid_idx] < -alpha] = 0\n",
    "\n",
    "Xdf = Xdf.loc[valid_idx].copy()\n",
    "print(\"Label distribution (Down/Neutral/Up):\", y3.value_counts().sort_index().to_dict())\n",
    "print(f\"Kept {len(valid_idx)} rows after horizon/smoothing trimming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa751ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological split\n",
    "split_idx = int(len(Xdf)*0.8)\n",
    "X_train, X_test = Xdf.iloc[:split_idx], Xdf.iloc[split_idx:]\n",
    "y_train, y_test = y3.iloc[:split_idx], y3.iloc[split_idx:]\n",
    "\n",
    "# StandardScaler fit on train, transform both (no leakage)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=Xdf.columns, index=X_train.index)\n",
    "X_test  = pd.DataFrame(scaler.transform(X_test),  columns=Xdf.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0f87e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train windows: 21574 | Test windows: 5390\n"
     ]
    }
   ],
   "source": [
    "class LOBDS3C(Dataset): # returns window dataset and label. label is assigned to window's end\n",
    "    def __init__(self, X, y_int, window=50, stride=10):\n",
    "        self.X = X.values.astype(np.float32)\n",
    "        self.y = y_int.values.astype(np.int64)\n",
    "        self.window = window; self.stride = stride\n",
    "        self.idxs = list(range(0, len(X) - window, stride))\n",
    "    def __len__(self): return len(self.idxs)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.idxs[idx]\n",
    "        x = self.X[s:s+self.window]\n",
    "        y = self.y[s + self.window - 1] # label at window end\n",
    "        return torch.from_numpy(x), torch.tensor(y)\n",
    "\n",
    "train_ds = LOBDS3C(X_train, y_train, window=window, stride=stride)\n",
    "test_ds  = LOBDS3C(X_test,  y_test,  window=window, stride=stride)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Train windows: {len(train_ds)} | Test windows: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e9e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation (based on input mode)\n",
    "class Inception(nn.Module): # time X feature inception mentioned in DeepLOB\n",
    "    def __init__(self, c_in, c_out=32):\n",
    "        super().__init__()\n",
    "        self.b1 = nn.Conv2d(c_in, c_out, (1,1))\n",
    "        self.b3 = nn.Conv2d(c_in, c_out, (1,3), padding=(0,1))\n",
    "        self.b5 = nn.Conv2d(c_in, c_out, (1,5), padding=(0,2))\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.b1(x), self.b3(x), self.b5(x)], dim=1)\n",
    "\n",
    "class DeepLOBRaw(nn.Module):\n",
    "    def __init__(self, lstm_hidden=64, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(1, 32, (1,2), stride=(1,2))\n",
    "        self.inc1 = Inception(32, c_out=32)\n",
    "        self.inc2 = Inception(96, c_out=32)\n",
    "        self.lstm = nn.LSTM(96, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        self.att  = nn.Linear(2*lstm_hidden, 1)\n",
    "        self.drop = nn.Dropout(dropout_p)\n",
    "        self.out  = nn.Linear(2*lstm_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.leaky_relu(self.stem(x))\n",
    "        x = F.leaky_relu(self.inc1(x))\n",
    "        x = F.leaky_relu(self.inc2(x))\n",
    "        x = x.mean(-1).permute(0,2,1)\n",
    "        h, _ = self.lstm(x)\n",
    "        w = torch.softmax(self.att(h).squeeze(-1), dim=1)\n",
    "        ctx = (h * w.unsqueeze(-1)).sum(1)\n",
    "        ctx = self.drop(ctx)\n",
    "        return self.out(ctx)\n",
    "\n",
    "class FeatTCN(nn.Module):\n",
    "    def __init__(self, f_in, lstm_hidden=64, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(f_in, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(64, lstm_hidden, batch_first=True, bidirectional=True)\n",
    "        self.att  = nn.Linear(2*lstm_hidden, 1)\n",
    "        self.drop = nn.Dropout(dropout_p)\n",
    "        self.out  = nn.Linear(2*lstm_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.tcn(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        h, _ = self.lstm(x)\n",
    "        w = torch.softmax(self.att(h).squeeze(-1), dim=1)\n",
    "        ctx = (h * w.unsqueeze(-1)).sum(1)\n",
    "        ctx = self.drop(ctx)\n",
    "        return self.out(ctx)\n",
    "\n",
    "# picking model based on mode\n",
    "if engineered_mode == \"raw\":\n",
    "    assert Xdf.shape[1] == 40, \"Raw mode expects exactly 40 LOB columns (10 levels x {ap,as,bp,bv}).\"\n",
    "    model = DeepLOBRaw(lstm_hidden=64, dropout_p=0.1).to(device)\n",
    "elif engineered_mode == \"feat\":\n",
    "    model = FeatTCN(f_in=Xdf.shape[1], lstm_hidden=64, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c9a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_counts = y_train.value_counts().reindex([0,1,2]).fillna(0).values.astype(np.float32)\n",
    "cls_weights = torch.tensor((cls_counts.sum() / np.clip(cls_counts, 1, None)), device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=cls_weights)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "sched = CyclicLR(opt, base_lr=3e-4, max_lr=1e-3,\n",
    "                 step_size_up=max(1, len(train_loader)//2), cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23d2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10  acc=0.361  loss=1.0931\n",
      "epoch 20  acc=0.394  loss=1.0849\n",
      "epoch 30  acc=0.441  loss=1.0595\n",
      "epoch 40  acc=0.481  loss=1.0251\n",
      "epoch 50  acc=0.529  loss=0.9754\n",
      "epoch 60  acc=0.569  loss=0.9300\n",
      "epoch 70  acc=0.605  loss=0.8811\n",
      "epoch 80  acc=0.636  loss=0.8329\n",
      "epoch 90  acc=0.659  loss=0.7909\n",
      "epoch 100  acc=0.676  loss=0.7567\n",
      "epoch 110  acc=0.695  loss=0.7146\n",
      "epoch 120  acc=0.718  loss=0.6791\n",
      "epoch 130  acc=0.729  loss=0.6517\n",
      "epoch 140  acc=0.748  loss=0.6142\n",
      "epoch 150  acc=0.752  loss=0.5990\n",
      "epoch 160  acc=0.767  loss=0.5642\n",
      "epoch 170  acc=0.776  loss=0.5426\n",
      "epoch 180  acc=0.792  loss=0.5080\n",
      "epoch 190  acc=0.805  loss=0.4845\n",
      "epoch 200  acc=0.808  loss=0.4728\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    tot, correct, run_loss = 0, 0, 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step(); sched.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            tot += yb.numel()\n",
    "            run_loss += loss.item() * yb.numel()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch:02d}  acc={correct/tot:.3f}  loss={run_loss/tot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43229d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows=true [Down,Neutral,Up], cols=pred):\n",
      "[[703 533 778]\n",
      " [536 396 525]\n",
      " [595 507 817]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.383     0.349     0.365      2014\n",
      "     Neutral      0.276     0.272     0.274      1457\n",
      "          Up      0.385     0.426     0.405      1919\n",
      "\n",
      "    accuracy                          0.355      5390\n",
      "   macro avg      0.348     0.349     0.348      5390\n",
      "weighted avg      0.355     0.355     0.355      5390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "probs_list, y_list = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        logits = model(xb.to(device)).cpu()\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        probs_list.append(probs)\n",
    "        y_list.append(yb)\n",
    "probs  = torch.cat(probs_list).numpy()\n",
    "y_true = torch.cat(y_list).numpy()\n",
    "y_pred = probs.argmax(1)\n",
    "\n",
    "# Confusion matrix & precision/recall report\n",
    "print(\"Confusion matrix (rows=true [Down,Neutral,Up], cols=pred):\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0,1,2]))\n",
    "print(classification_report(y_true, y_pred, labels=[0,1,2],\n",
    "                            target_names=[\"Down\",\"Neutral\",\"Up\"], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16aa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
